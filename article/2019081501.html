<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>TensorFlow基础知识 | foochane</title><meta name="description" content="TensorFlow基础知识"><meta name="keywords" content="TensorFlow"><meta name="author" content="foochane"><meta name="copyright" content="foochane"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/images/site/favicon.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://hm.baidu.com"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="TensorFlow基础知识"><meta name="twitter:description" content="TensorFlow基础知识"><meta name="twitter:image" content="https://foochane.cn/images/cover/3.jpg"><meta property="og:type" content="article"><meta property="og:title" content="TensorFlow基础知识"><meta property="og:url" content="https://foochane.cn/article/2019081501"><meta property="og:site_name" content="foochane"><meta property="og:description" content="TensorFlow基础知识"><meta property="og:image" content="https://foochane.cn/images/cover/3.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://foochane.cn/article/2019081501"><link rel="prev" title="文本分类之文本预处理" href="https://foochane.cn/article/2019090101.html"><link rel="next" title="IDEA下使用Spark连接Hive" href="https://foochane.cn/article/2019071701.html"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?e75706f2332bc6bbea2cc9a3372553f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: true,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="foochane" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">foochane</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/images/site/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">8</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-综述"><span class="toc_mobile_items-text">1 综述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-张量-Tensor"><span class="toc_mobile_items-text">2 张量(Tensor)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-计算图"><span class="toc_mobile_items-text">2 计算图</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#3-会话"><span class="toc_mobile_items-text">3 会话</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4-变量"><span class="toc_mobile_items-text">4 变量</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-1-变量创建"><span class="toc_mobile_items-text">4.1 变量创建</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-2-变量初始化"><span class="toc_mobile_items-text">4.2 变量初始化</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#4-2-1-tf-initialize-all-variables"><span class="toc_mobile_items-text">4.2.1 tf.initialize_all_variables()</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#4-2-2-initialized-value"><span class="toc_mobile_items-text">4.2.2 initialized_value()</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-3-保存和加载"><span class="toc_mobile_items-text">4.3 保存和加载</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#4-3-1-保存变量"><span class="toc_mobile_items-text">4.3.1 保存变量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#4-3-2-恢复变量"><span class="toc_mobile_items-text">4.3.2 恢复变量</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5-Fetch和Feed"><span class="toc_mobile_items-text">5 Fetch和Feed</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-1-Fetch"><span class="toc_mobile_items-text">5.1 Fetch</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-2-Feed"><span class="toc_mobile_items-text">5.2 Feed</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="color"></div><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-综述"><span class="toc-text">1 综述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-张量-Tensor"><span class="toc-text">2 张量(Tensor)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-计算图"><span class="toc-text">2 计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-会话"><span class="toc-text">3 会话</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-变量"><span class="toc-text">4 变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-变量创建"><span class="toc-text">4.1 变量创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-变量初始化"><span class="toc-text">4.2 变量初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-tf-initialize-all-variables"><span class="toc-text">4.2.1 tf.initialize_all_variables()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-initialized-value"><span class="toc-text">4.2.2 initialized_value()</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-保存和加载"><span class="toc-text">4.3 保存和加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-保存变量"><span class="toc-text">4.3.1 保存变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-恢复变量"><span class="toc-text">4.3.2 恢复变量</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Fetch和Feed"><span class="toc-text">5 Fetch和Feed</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Fetch"><span class="toc-text">5.1 Fetch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Feed"><span class="toc-text">5.2 Feed</span></a></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/images/cover/3.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">TensorFlow基础知识</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-08-15</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/TensorFlow/">TensorFlow</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.4k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 9 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span><span class="post-meta__separator">|</span><i class="fa fa-comments-o post-meta__icon fa-fw" aria-hidden="true"></i><span>评论数:</span><a href="/article/2019081501.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/article/2019081501.html" itemprop="commentCount"></span></a></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="1-综述"><a href="#1-综述" class="headerlink" title="1 综述"></a>1 综述</h2><p>TensorFlow的编程系统中：</p>
<ol>
<li>使用张量(<code>tensor</code>)来表示数据</li>
<li>使用图(<code>graph</code>)来表示计算任务。 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组。</li>
<li>使用会话(<code>Session</code>)的上下文(<code>context</code>)中执行图</li>
<li>通过变量(<code>Variable</code>)维护状态</li>
<li>使用 <code>feed</code> 和 <code>fetch</code> 赋值和获取数据.</li>
</ol>
<h2 id="2-张量-Tensor"><a href="#2-张量-Tensor" class="headerlink" title="2 张量(Tensor)"></a>2 张量(Tensor)</h2><blockquote>
<p>张量就是多维数组(列表),用“阶”表示张量的维度。</p>
</blockquote>
<ul>
<li>0阶张量称作标量(scalar),表示一个单独的数;举例 S=123</li>
<li>1阶张量称作向量(vector),表示一个一维数组;举例 V=[1,2,3]</li>
<li>2阶张量称作矩阵(matrix),表示一个二维数组,它可以有 i 行 j 列个元素,每个元素可<br>以用行号和列号共同索引到;<br>举例 m=[[1, 2, 3], [4, 5, 6], [7, 8, 9]]</li>
</ul>
<blockquote>
<p>判断张量是几阶的,就通过张量右边的方括号数,0 个是 0 阶,n 个是 n 阶。例如 t=[ [ [… ] ] ]为 3 阶。</p>
</blockquote>
<p>代码演示：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">b=tf.constant([<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">result=a+b</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></div>

<p>输出的结果为：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"add:0"</span>, shape=(<span class="number">2</span>,), dtype=float32)</span><br></pre></td></tr></table></figure></div>

<p>解释：</p>
<ul>
<li>add ：节点名</li>
<li>shape :维度信息，括号里只有一个数“2“，表示维度是1且一个维度里有两个元素</li>
<li>dtpye :数据类型</li>
</ul>
<p>另：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c=tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])  <span class="comment">#Tensor("Const_2:0", shape=(2,), dtype=float32) 是个向量，有两个元素</span></span><br><span class="line">d=tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>]])  <span class="comment">#Tensor("Const_3:0", shape=(1, 2), dtype=float32) 1行2列矩阵</span></span><br><span class="line">e=tf.constant([[<span class="number">1.0</span>],[<span class="number">2.0</span>]])  <span class="comment">#Tensor("Const_4:0", shape=(2, 1), dtype=float32) 2行1列矩阵</span></span><br></pre></td></tr></table></figure></div>
<h2 id="2-计算图"><a href="#2-计算图" class="headerlink" title="2 计算图"></a>2 计算图</h2><blockquote>
<p>计算图(Graph):搭建神经网络的计算过程,是承载一个或多个计算节点的一张图,只搭建网络,不运算。</p>
</blockquote>
<p><a href="https://raw.githubusercontent.com/foochane/Tensorflow-Learning/master/image/jisuantu.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="计算图" class="fancybox"><img alt="计算图" title="计算图" data-src="https://raw.githubusercontent.com/foochane/Tensorflow-Learning/master/image/jisuantu.png" class="lazyload"></a></p>
<p> x1、x2 表示输入,w1、w2 分别是 x1 到 y 和 x2 到 y 的权重,y=x1<em>w1+x2</em>w2。</p>
<p> 我们实现上述计算图:<br> <div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment">#引入模块</span></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]]) <span class="comment">#定义一个 2 阶张量等于[[1.0,2.0]]</span></span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]]) <span class="comment">#定义一个 2 阶张量等于[[3.0],[4.0]]</span></span><br><span class="line">y = tf.matmul(x, w) <span class="comment">#实现 xw 矩阵乘法</span></span><br><span class="line"><span class="keyword">print</span> y <span class="comment">#打印出结果</span></span><br></pre></td></tr></table></figure></div></p>
<p>可以打印出这样一句话:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(“matmul:<span class="number">0</span>”, shape(<span class="number">1</span>,<span class="number">1</span>), dtype=float32),</span><br></pre></td></tr></table></figure></div>
<p>从这里我们可以看出,print 的结果显示 y 是一个张量,只搭建承载计算过程的<br>计算图,并没有运算,如果我们想得到运算结果就要用到“会话 Session()”了。</p>
<h2 id="3-会话"><a href="#3-会话" class="headerlink" title="3 会话"></a>3 会话</h2><blockquote>
<p>会话(Session):执行计算图中的节点运算。</p>
</blockquote>
<p>我们用 with 结构实现,语法如下:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure></div>
<p>【举例】</p>
<p>我们执行 Session()会话可得到矩阵相乘结果:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment">#引入模块</span></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]]) <span class="comment">#定义一个 2 阶张量等于[[1.0,2.0]]</span></span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]]) <span class="comment">#定义一个 2 阶张量等于[[3.0],[4.0]]</span></span><br><span class="line">y = tf.matmul(x, w) <span class="comment">#实现 xw 矩阵乘法</span></span><br><span class="line"><span class="keyword">print</span> y <span class="comment">#打印出结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y)) <span class="comment">#执行会话并打印出执行后的结果</span></span><br></pre></td></tr></table></figure></div>

<p>可以打印出这样的结果:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(“matmul:<span class="number">0</span>”, shape(<span class="number">1</span>,<span class="number">1</span>), dtype=float32)</span><br><span class="line">[[<span class="number">11.</span>]]</span><br></pre></td></tr></table></figure></div>
<p>我们可以看到,运行 Session()会话前只打印出 y 是个张量的提示,运行 Session()<br>会话后打印出了 y 的结果 1.0 * 3.0 + 2.0 * 4.0 = 11.0。</p>
<h2 id="4-变量"><a href="#4-变量" class="headerlink" title="4 变量"></a>4 变量</h2><h3 id="4-1-变量创建"><a href="#4-1-变量创建" class="headerlink" title="4.1 变量创建"></a>4.1 变量创建</h3><p><a href="http://www.tensorfly.cn/tfdoc/api_docs/python/state_ops.html" target="_blank" rel="noopener">变量</a>的创建使用一个张量作为初始值传入构造函数Variable()，初始值是常量或是随机值。</p>
<p>注意，所有这些操作符都需要你指定张量的shape。那个形状自动成为变量的shape。变量的shape通常是固定的，但TensorFlow提供了高级的机制来重新调整其行列数</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个变量, 初始化为标量 0.</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重和偏置</span></span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),name=<span class="string">"weights"</span>)</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</span><br></pre></td></tr></table></figure></div>

<p>常用的生成随机数/数组的函数有:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.random_normal()   #生成正态分布随机数</span><br><span class="line">tf.truncated_normal() #生成去掉过大偏离点的正态分布随机数</span><br><span class="line">tf.random_uniform()  #生成均匀分布随机数</span><br><span class="line">tf.zeros         #表示生成全 0 数组</span><br><span class="line">tf.ones       #表示生成全 1 数组</span><br><span class="line">tf.fill     #表示生成全定值数组</span><br><span class="line">tf.constant #表示生成直接给定值的数组</span><br></pre></td></tr></table></figure></div>

<p>具体可以查看：<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/constant_op.html" target="_blank" rel="noopener">http://www.tensorfly.cn/tfdoc/api_docs/python/constant_op.html</a></p>
<h3 id="4-2-变量初始化"><a href="#4-2-变量初始化" class="headerlink" title="4.2 变量初始化"></a>4.2 变量初始化</h3><blockquote>
<p>变量的初始化必须在模型的其它操作运行之前完成</p>
</blockquote>
<p>最简单的方法就是添加一个给所有变量初始化的操作，并在使用模型之前首先运行那个操作。</p>
<h4 id="4-2-1-tf-initialize-all-variables"><a href="#4-2-1-tf-initialize-all-variables" class="headerlink" title="4.2.1 tf.initialize_all_variables()"></a>4.2.1 tf.initialize_all_variables()</h4><p>使用tf.initialize_all_variables()添加一个操作对变量做初始化。记得在完全构建好模型并加载之后再运行那个操作。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create two variables.</span></span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</span><br><span class="line">                      name=<span class="string">"weights"</span>)</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=<span class="string">"biases"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># Add an op to initialize the variables.</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Later, when launching the model</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Run the init operation.</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment"># Use the model</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></div>
<h4 id="4-2-2-initialized-value"><a href="#4-2-2-initialized-value" class="headerlink" title="4.2.2 initialized_value()"></a>4.2.2 initialized_value()</h4><p>。由于tf.initialize_all_variables()是并行地初始化所有变量，有时候会需要用另一个变量的初始化值给当前变量初始化。</p>
<p>用其它变量的值初始化一个新的变量时，使用其它变量的initialized_value()属性。你可以直接把已初始化的值作为新变量的初始值，或者把它当做tensor计算得到一个值赋予新变量。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a variable with a random value.</span></span><br><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>),</span><br><span class="line">                      name=<span class="string">"weights"</span>)</span><br><span class="line"><span class="comment"># Create another variable with the same value as 'weights'.</span></span><br><span class="line">w2 = tf.Variable(weights.initialized_value(), name=<span class="string">"w2"</span>)</span><br><span class="line"><span class="comment"># Create another variable with twice the value of 'weights'</span></span><br><span class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">0.2</span>, name=<span class="string">"w_twice"</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="4-3-保存和加载"><a href="#4-3-保存和加载" class="headerlink" title="4.3 保存和加载"></a>4.3 保存和加载</h3><h4 id="4-3-1-保存变量"><a href="#4-3-1-保存变量" class="headerlink" title="4.3.1 保存变量"></a>4.3.1 保存变量</h4><p>最简单的保存和恢复模型的方法是使用<code>tf.train.Saver</code>对象。构造器给graph的所有变量，或是定义在列表里的变量，添加save和restore ops。saver对象提供了方法来运行这些ops，定义检查点文件的读写路径。</p>
<p>变量存储在二进制文件里，主要包含从变量名到tensor值的映射关系。</p>
<p>当你创建一个Saver对象时，你可以选择性地为检查点文件中的变量定义变量名。默认情况下，将使用每个变量Variable.name属性的值。</p>
<p>保存变量时，用tf.train.Saver()创建一个Saver来管理模型中的所有变量。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables.</span></span><br><span class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># Add an op to initialize the variables.</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add ops to save and restore all the variables.</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Later, launch the model, initialize the variables, do some work, save the</span></span><br><span class="line"><span class="comment"># variables to disk.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  <span class="comment"># Do some work with the model.</span></span><br><span class="line">  ..</span><br><span class="line">  <span class="comment"># Save the variables to disk.</span></span><br><span class="line">  save_path = saver.save(sess, <span class="string">"/tmp/model.ckpt"</span>)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">"Model saved in file: "</span>, save_path</span><br></pre></td></tr></table></figure></div>

<h4 id="4-3-2-恢复变量"><a href="#4-3-2-恢复变量" class="headerlink" title="4.3.2 恢复变量"></a>4.3.2 恢复变量</h4><p>用同一个Saver对象来恢复变量。注意，当你从文件中恢复变量时，不需要事先对它们做初始化。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create some variables.</span></span><br><span class="line">v1 = tf.Variable(..., name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(..., name=<span class="string">"v2"</span>)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># Add ops to save and restore all the variables.</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Later, launch the model, use the saver to restore variables from disk, and</span></span><br><span class="line"><span class="comment"># do some work with the model.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Restore variables from disk.</span></span><br><span class="line">  saver.restore(sess, <span class="string">"/tmp/model.ckpt"</span>)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">"Model restored."</span></span><br><span class="line">  <span class="comment"># Do some work with the model</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></div>

<h2 id="5-Fetch和Feed"><a href="#5-Fetch和Feed" class="headerlink" title="5 Fetch和Feed"></a>5 Fetch和Feed</h2><h3 id="5-1-Fetch"><a href="#5-1-Fetch" class="headerlink" title="5.1 Fetch"></a>5.1 Fetch</h3><p>为了取回操作的输出内容, 可以在使用 Session 对象的 run() 调用 执行图时, 传入一些 tensor, 这些 tensor 会帮助你取回结果，也就是Fetch操作。</p>
<p>示例代码：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个变量, 初始化为标量 0.</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 op, 其作用是使 state 增加 1</span></span><br><span class="line"></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图后, 变量必须先经过`初始化` (init) op 初始化,</span></span><br><span class="line"><span class="comment"># 首先必须增加一个`初始化` op 到图中.</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图, 运行 op</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 运行 'init' op</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  <span class="comment"># 打印 'state' 的初始值</span></span><br><span class="line">  <span class="keyword">print</span> sess.run(state)</span><br><span class="line">  <span class="comment"># 运行 op, 更新 'state', 并打印 'state'</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    sess.run(update)</span><br><span class="line">    <span class="keyword">print</span> sess.run(state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure></div>
<p>在之前的例子里, 我们只取回了单个节点 state, 但是你也可以取回多个 tensor:</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">input2 = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">input3 = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">intermed = tf.add(input2, input3)</span><br><span class="line">mul = tf.mul(input1, intermed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session():</span><br><span class="line">  result = sess.run([mul, intermed])</span><br><span class="line">  <span class="keyword">print</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]</span></span><br></pre></td></tr></table></figure></div>
<p>需要获取的多个 tensor 值，在 op 的一次运行中一起获得（而不是逐个去获取 tensor）。</p>
<h3 id="5-2-Feed"><a href="#5-2-Feed" class="headerlink" title="5.2 Feed"></a>5.2 Feed</h3><p>上述示例在计算图中引入了 tensor, 以常量或变量的形式存储. TensorFlow 还提供了 feed 机制, 该机制 可以临时替代图中的任意操作中的 tensor 可以对图中任何操作提交补丁, 直接插入一个tensor.</p>
<p>feed 使用一个 tensor 值临时替换一个操作的输出结果. 你可以提供 feed 数据作为 run() 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失. 最常见的用例是将某些特殊的操作指定为 “feed” 操作, 标记的方法是使用 tf.placeholder() 为这些操作创建占位符.</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.types.float32)</span><br><span class="line">input2 = tf.placeholder(tf.types.float32)</span><br><span class="line">output = tf.mul(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="keyword">print</span> sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># [array([ 14.], dtype=float32)]</span></span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>参考： </p>
</blockquote>
<ol>
<li><a href="http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html" target="_blank" rel="noopener">http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html</a></li>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">https://www.tensorflow.org/</a></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">foochane</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://foochane.cn/article/2019081501.html">https://foochane.cn/article/2019081501.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://foochane.cn">foochane</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/TensorFlow/">TensorFlow    </a></div><div class="post_share"><div class="social-share" data-image="/images/cover/3.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/images/site/reward/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/images/site/reward/alipay.png" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/article/2019090101.html"><img class="prev_cover lazyload" data-src="/images/cover/2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>文本分类之文本预处理</span></div></a></div><div class="next-post pull_right"><a href="/article/2019071701.html"><img class="next_cover lazyload" data-src="/images/cover/1.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>IDEA下使用Spark连接Hive</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = true == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'zCeKCftRr91sIth6b8UsWKb9-gzGzoHsz',
  appKey:'SIMyV1MuzsbUpvUErd6bUlC2',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></main><footer id="footer" style="background-image: url(/images/cover/3.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By foochane</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/click_heart.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>