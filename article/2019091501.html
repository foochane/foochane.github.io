<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>文本分类之常用方法总结 | foochane</title><meta name="description" content="文本分类之常用方法总结"><meta name="keywords" content="文本分类"><meta name="author" content="foochane"><meta name="copyright" content="foochane"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/images/site/favicon.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://hm.baidu.com"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="文本分类之常用方法总结"><meta name="twitter:description" content="文本分类之常用方法总结"><meta name="twitter:image" content="https://foochane.cntrue"><meta property="og:type" content="article"><meta property="og:title" content="文本分类之常用方法总结"><meta property="og:url" content="https://foochane.cn/article/2019091501"><meta property="og:site_name" content="foochane"><meta property="og:description" content="文本分类之常用方法总结"><meta property="og:image" content="https://foochane.cntrue"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://foochane.cn/article/2019091501"><link rel="prev" title="pip源更换国内镜像" href="https://foochane.cn/article/2019102201.html"><link rel="next" title="文本分类之CountVectorizer使用" href="https://foochane.cn/article/2019090301.html"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?e75706f2332bc6bbea2cc9a3372553f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: true,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="foochane" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">foochane</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/images/site/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">8</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-TFIDF-逻辑回归"><span class="toc_mobile_items-text">1 TFIDF+逻辑回归</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-1-导入数据"><span class="toc_mobile_items-text">1.1 导入数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-2-标签转换为数字"><span class="toc_mobile_items-text">1.2 标签转换为数字</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-3-TF-IDF提取文本特征"><span class="toc_mobile_items-text">1.3 TF-IDF提取文本特征</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-4-切分数据"><span class="toc_mobile_items-text">1.4 切分数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-5-使用模型分类"><span class="toc_mobile_items-text">1.5 使用模型分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-WordCounts特征-逻辑回归"><span class="toc_mobile_items-text">2 WordCounts特征+逻辑回归</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-1-导入数据"><span class="toc_mobile_items-text">2.1 导入数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-2-标签转换为数字"><span class="toc_mobile_items-text">2.2 标签转换为数字</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-3-提取的word-counts特征"><span class="toc_mobile_items-text">2.3 提取的word counts特征</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-4-切分数据集"><span class="toc_mobile_items-text">2.4 切分数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-5-定义损失函数"><span class="toc_mobile_items-text">2.5 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-6-使用模型分类"><span class="toc_mobile_items-text">2.6 使用模型分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#3-TF-IDF-朴素贝叶斯"><span class="toc_mobile_items-text">3 TF-IDF+朴素贝叶斯</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-1-导入数据"><span class="toc_mobile_items-text">3.1 导入数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-2-标签转换为数字"><span class="toc_mobile_items-text">3.2 标签转换为数字</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-3-提取的word-counts特征"><span class="toc_mobile_items-text">3.3 提取的word counts特征</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-4-切分数据集"><span class="toc_mobile_items-text">3.4 切分数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-5-定义损失函数"><span class="toc_mobile_items-text">3.5 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-6-使用模型分类"><span class="toc_mobile_items-text">3.6 使用模型分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4-WordCounts特征-朴素贝叶斯"><span class="toc_mobile_items-text">4 WordCounts特征+朴素贝叶斯</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-1-导入数据"><span class="toc_mobile_items-text">4.1 导入数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-2-标签转换为数字"><span class="toc_mobile_items-text">4.2 标签转换为数字</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-3-提取的word-counts特征"><span class="toc_mobile_items-text">4.3 提取的word counts特征</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-4-切分数据集"><span class="toc_mobile_items-text">4.4 切分数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-5-定义损失函数"><span class="toc_mobile_items-text">4.5 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-6-使用模型分类"><span class="toc_mobile_items-text">4.6 使用模型分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5-TF-IDF的SVD特征-支持向量机"><span class="toc_mobile_items-text">5 TF-IDF的SVD特征+支持向量机</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-1-数据准备"><span class="toc_mobile_items-text">5.1 数据准备</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-2-定义损失函数"><span class="toc_mobile_items-text">5.2 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-3-数据标准化"><span class="toc_mobile_items-text">5.3 数据标准化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-4-使用模型分类"><span class="toc_mobile_items-text">5.4 使用模型分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#6-网格搜索-朴素贝叶斯"><span class="toc_mobile_items-text">6 网格搜索+朴素贝叶斯</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#6-1-准备数据"><span class="toc_mobile_items-text">6.1 准备数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#6-2-定义损失函数"><span class="toc_mobile_items-text">6.2 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#6-3-创建评分函数"><span class="toc_mobile_items-text">6.3 创建评分函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#6-4-进行网格搜索"><span class="toc_mobile_items-text">6.4 进行网格搜索</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#6-5-用模型来分类"><span class="toc_mobile_items-text">6.5 用模型来分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#7-word2vec的词嵌入-xgboost"><span class="toc_mobile_items-text">7 word2vec的词嵌入+xgboost</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#7-1-数据准备"><span class="toc_mobile_items-text">7.1 数据准备</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#7-2-构建word2vec模型"><span class="toc_mobile_items-text">7.2 构建word2vec模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#7-3-word2vec模型的简单使用"><span class="toc_mobile_items-text">7.3 word2vec模型的简单使用</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7-3-1-构建词建词嵌入字典"><span class="toc_mobile_items-text">7.3.1 构建词建词嵌入字典</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7-3-2-获取某个词的向量"><span class="toc_mobile_items-text">7.3.2 获取某个词的向量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7-3-3-查看某个词的与其他词的相似度"><span class="toc_mobile_items-text">7.3.3 查看某个词的与其他词的相似度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7-3-4-保存模型"><span class="toc_mobile_items-text">7.3.4 保存模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#7-3-5-加载模型"><span class="toc_mobile_items-text">7.3.5 加载模型</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#7-4-训练数据处理"><span class="toc_mobile_items-text">7.4 训练数据处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#7-5-调用模型进行分类"><span class="toc_mobile_items-text">7.5 调用模型进行分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#8-WordCounts特征-xgboost"><span class="toc_mobile_items-text">8 WordCounts特征+xgboost</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-1-导入数据"><span class="toc_mobile_items-text">8.1 导入数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-2-标签转换为数字"><span class="toc_mobile_items-text">8.2 标签转换为数字</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-3-提取的word-counts特征"><span class="toc_mobile_items-text">8.3 提取的word counts特征</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-4-切分数据集"><span class="toc_mobile_items-text">8.4 切分数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-5-定义损失函数"><span class="toc_mobile_items-text">8.5 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-6-使用模型分类"><span class="toc_mobile_items-text">8.6 使用模型分类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#9-TF-IDF的SVD特征-xgboost"><span class="toc_mobile_items-text">9 TF-IDF的SVD特征+xgboost</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#9-1-数据准备"><span class="toc_mobile_items-text">9.1 数据准备</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#9-2-数据标准化"><span class="toc_mobile_items-text">9.2 数据标准化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#9-3-定义损失函数"><span class="toc_mobile_items-text">9.3 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#9-4-使用模型分类"><span class="toc_mobile_items-text">9.4 使用模型分类</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#9-4-1-基于tf-idf的svd特征"><span class="toc_mobile_items-text">9.4.1 基于tf-idf的svd特征</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#9-4-2-标准化的tf-idf-svd特征"><span class="toc_mobile_items-text">9.4.2 标准化的tf-idf-svd特征</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#10-TF-IDF-xgboost"><span class="toc_mobile_items-text">10 TF-IDF+xgboost</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-1-数据准备"><span class="toc_mobile_items-text">10.1 数据准备</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-2-定义损失函数"><span class="toc_mobile_items-text">10.2 定义损失函数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-3-使用模型分类"><span class="toc_mobile_items-text">10.3 使用模型分类</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="color"></div><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-TFIDF-逻辑回归"><span class="toc-text">1 TFIDF+逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-导入数据"><span class="toc-text">1.1 导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-标签转换为数字"><span class="toc-text">1.2 标签转换为数字</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-TF-IDF提取文本特征"><span class="toc-text">1.3 TF-IDF提取文本特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-切分数据"><span class="toc-text">1.4 切分数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-使用模型分类"><span class="toc-text">1.5 使用模型分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-WordCounts特征-逻辑回归"><span class="toc-text">2 WordCounts特征+逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-导入数据"><span class="toc-text">2.1 导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-标签转换为数字"><span class="toc-text">2.2 标签转换为数字</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-提取的word-counts特征"><span class="toc-text">2.3 提取的word counts特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-切分数据集"><span class="toc-text">2.4 切分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-定义损失函数"><span class="toc-text">2.5 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-使用模型分类"><span class="toc-text">2.6 使用模型分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-TF-IDF-朴素贝叶斯"><span class="toc-text">3 TF-IDF+朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-导入数据"><span class="toc-text">3.1 导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-标签转换为数字"><span class="toc-text">3.2 标签转换为数字</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-提取的word-counts特征"><span class="toc-text">3.3 提取的word counts特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-切分数据集"><span class="toc-text">3.4 切分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-定义损失函数"><span class="toc-text">3.5 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-使用模型分类"><span class="toc-text">3.6 使用模型分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-WordCounts特征-朴素贝叶斯"><span class="toc-text">4 WordCounts特征+朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-导入数据"><span class="toc-text">4.1 导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-标签转换为数字"><span class="toc-text">4.2 标签转换为数字</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-提取的word-counts特征"><span class="toc-text">4.3 提取的word counts特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-切分数据集"><span class="toc-text">4.4 切分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-定义损失函数"><span class="toc-text">4.5 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-使用模型分类"><span class="toc-text">4.6 使用模型分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-TF-IDF的SVD特征-支持向量机"><span class="toc-text">5 TF-IDF的SVD特征+支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-数据准备"><span class="toc-text">5.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-定义损失函数"><span class="toc-text">5.2 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-数据标准化"><span class="toc-text">5.3 数据标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-使用模型分类"><span class="toc-text">5.4 使用模型分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-网格搜索-朴素贝叶斯"><span class="toc-text">6 网格搜索+朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-准备数据"><span class="toc-text">6.1 准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-定义损失函数"><span class="toc-text">6.2 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-创建评分函数"><span class="toc-text">6.3 创建评分函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-进行网格搜索"><span class="toc-text">6.4 进行网格搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-用模型来分类"><span class="toc-text">6.5 用模型来分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-word2vec的词嵌入-xgboost"><span class="toc-text">7 word2vec的词嵌入+xgboost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-数据准备"><span class="toc-text">7.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-构建word2vec模型"><span class="toc-text">7.2 构建word2vec模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-word2vec模型的简单使用"><span class="toc-text">7.3 word2vec模型的简单使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-1-构建词建词嵌入字典"><span class="toc-text">7.3.1 构建词建词嵌入字典</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-2-获取某个词的向量"><span class="toc-text">7.3.2 获取某个词的向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-3-查看某个词的与其他词的相似度"><span class="toc-text">7.3.3 查看某个词的与其他词的相似度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-4-保存模型"><span class="toc-text">7.3.4 保存模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-5-加载模型"><span class="toc-text">7.3.5 加载模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-训练数据处理"><span class="toc-text">7.4 训练数据处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-5-调用模型进行分类"><span class="toc-text">7.5 调用模型进行分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-WordCounts特征-xgboost"><span class="toc-text">8 WordCounts特征+xgboost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-导入数据"><span class="toc-text">8.1 导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-标签转换为数字"><span class="toc-text">8.2 标签转换为数字</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-提取的word-counts特征"><span class="toc-text">8.3 提取的word counts特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-切分数据集"><span class="toc-text">8.4 切分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-定义损失函数"><span class="toc-text">8.5 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-6-使用模型分类"><span class="toc-text">8.6 使用模型分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-TF-IDF的SVD特征-xgboost"><span class="toc-text">9 TF-IDF的SVD特征+xgboost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-数据准备"><span class="toc-text">9.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-数据标准化"><span class="toc-text">9.2 数据标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-定义损失函数"><span class="toc-text">9.3 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-使用模型分类"><span class="toc-text">9.4 使用模型分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-4-1-基于tf-idf的svd特征"><span class="toc-text">9.4.1 基于tf-idf的svd特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-4-2-标准化的tf-idf-svd特征"><span class="toc-text">9.4.2 标准化的tf-idf-svd特征</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-TF-IDF-xgboost"><span class="toc-text">10 TF-IDF+xgboost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-数据准备"><span class="toc-text">10.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-定义损失函数"><span class="toc-text">10.2 定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-使用模型分类"><span class="toc-text">10.3 使用模型分类</span></a></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container"><div id="post-info"><div id="post-title"><div class="posttitle">文本分类之常用方法总结</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-09-15</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/NLP/">NLP</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">4.7k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 23 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span><span class="post-meta__separator">|</span><i class="fa fa-comments-o post-meta__icon fa-fw" aria-hidden="true"></i><span>评论数:</span><a href="/article/2019091501.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/article/2019091501.html" itemprop="commentCount"></span></a></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="1-TFIDF-逻辑回归"><a href="#1-TFIDF-逻辑回归" class="headerlink" title="1 TFIDF+逻辑回归"></a>1 TFIDF+逻辑回归</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span>  preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure></div>

<h3 id="1-1-导入数据"><a href="#1-1-导入数据" class="headerlink" title="1.1 导入数据"></a>1.1 导入数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())</span><br></pre></td></tr></table></figure></div>

<h3 id="1-2-标签转换为数字"><a href="#1-2-标签转换为数字" class="headerlink" title="1.2 标签转换为数字"></a>1.2 标签转换为数字</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br></pre></td></tr></table></figure></div>

<h3 id="1-3-TF-IDF提取文本特征"><a href="#1-3-TF-IDF提取文本特征" class="headerlink" title="1.3 TF-IDF提取文本特征"></a>1.3 TF-IDF提取文本特征</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">tfv1 = TfidfVectorizer(min_df=<span class="number">4</span>,  </span><br><span class="line">                       max_df=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用TF-IDF来fit训练集和测试集（半监督学习）</span></span><br><span class="line">tfv1.fit(text)</span><br><span class="line">features = tfv1.transform(text)</span><br></pre></td></tr></table></figure></div>

<p>查看分词数目</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">len(tfv1.get_feature_names())</span><br></pre></td></tr></table></figure></div>



<pre><code>84412</code></pre><h3 id="1-4-切分数据"><a href="#1-4-切分数据" class="headerlink" title="1.4 切分数据"></a>1.4 切分数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_tfv, x_valid_tfv, y_train, y_valid = train_test_split(features, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="1-5-使用模型分类"><a href="#1-5-使用模型分类" class="headerlink" title="1.5 使用模型分类"></a>1.5 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用提取的TFIDF特征来fit一个简单的Logistic Regression </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(C=<span class="number">1.0</span>,solver=<span class="string">'lbfgs'</span>,multi_class=<span class="string">'multinomial'</span>)</span><br><span class="line">clf.fit(x_train_tfv, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_tfv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.564 


/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)</code></pre><h2 id="2-WordCounts特征-逻辑回归"><a href="#2-WordCounts特征-逻辑回归" class="headerlink" title="2 WordCounts特征+逻辑回归"></a>2 WordCounts特征+逻辑回归</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br></pre></td></tr></table></figure></div>

<h3 id="2-1-导入数据"><a href="#2-1-导入数据" class="headerlink" title="2.1 导入数据"></a>2.1 导入数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())</span><br></pre></td></tr></table></figure></div>

<h3 id="2-2-标签转换为数字"><a href="#2-2-标签转换为数字" class="headerlink" title="2.2 标签转换为数字"></a>2.2 标签转换为数字</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br></pre></td></tr></table></figure></div>

<h3 id="2-3-提取的word-counts特征"><a href="#2-3-提取的word-counts特征" class="headerlink" title="2.3 提取的word counts特征"></a>2.3 提取的word counts特征</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ctv = CountVectorizer(min_df=<span class="number">3</span>,</span><br><span class="line">                      max_df=<span class="number">0.5</span>,</span><br><span class="line">                      ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Count Vectorizer来fit训练集和测试集（半监督学习）</span></span><br><span class="line">ctv.fit(text)</span><br><span class="line">text_ctv = ctv.transform(text)</span><br></pre></td></tr></table></figure></div>

<h3 id="2-4-切分数据集"><a href="#2-4-切分数据集" class="headerlink" title="2.4 切分数据集"></a>2.4 切分数据集</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_ctv, x_valid_ctv, y_train, y_valid = train_test_split(text_ctv, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="2-5-定义损失函数"><a href="#2-5-定义损失函数" class="headerlink" title="2.5 定义损失函数"></a>2.5 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="2-6-使用模型分类"><a href="#2-6-使用模型分类" class="headerlink" title="2.6 使用模型分类"></a>2.6 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用提取的word counts特征来fit一个简单的Logistic Regression </span></span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(C=<span class="number">1.0</span>,solver=<span class="string">'lbfgs'</span>,multi_class=<span class="string">'multinomial'</span>)</span><br><span class="line">clf.fit(x_train_ctv, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_ctv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.784 


/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)</code></pre><h2 id="3-TF-IDF-朴素贝叶斯"><a href="#3-TF-IDF-朴素贝叶斯" class="headerlink" title="3 TF-IDF+朴素贝叶斯"></a>3 TF-IDF+朴素贝叶斯</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br></pre></td></tr></table></figure></div>

<h3 id="3-1-导入数据"><a href="#3-1-导入数据" class="headerlink" title="3.1 导入数据"></a>3.1 导入数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())</span><br></pre></td></tr></table></figure></div>

<h3 id="3-2-标签转换为数字"><a href="#3-2-标签转换为数字" class="headerlink" title="3.2 标签转换为数字"></a>3.2 标签转换为数字</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br></pre></td></tr></table></figure></div>

<h3 id="3-3-提取的word-counts特征"><a href="#3-3-提取的word-counts特征" class="headerlink" title="3.3 提取的word counts特征"></a>3.3 提取的word counts特征</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ctv = CountVectorizer(min_df=<span class="number">3</span>,</span><br><span class="line">                      max_df=<span class="number">0.5</span>,</span><br><span class="line">                      ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Count Vectorizer来fit训练集和测试集（半监督学习）</span></span><br><span class="line">ctv.fit(text)</span><br><span class="line">text_ctv = ctv.transform(text)</span><br></pre></td></tr></table></figure></div>

<h3 id="3-4-切分数据集"><a href="#3-4-切分数据集" class="headerlink" title="3.4 切分数据集"></a>3.4 切分数据集</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_ctv, x_valid_ctv, y_train, y_valid = train_test_split(text_ctv, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="3-5-定义损失函数"><a href="#3-5-定义损失函数" class="headerlink" title="3.5 定义损失函数"></a>3.5 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="3-6-使用模型分类"><a href="#3-6-使用模型分类" class="headerlink" title="3.6 使用模型分类"></a>3.6 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用提取的word counts特征来fit一个简单的Logistic Regression </span></span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(C=<span class="number">1.0</span>,solver=<span class="string">'lbfgs'</span>,multi_class=<span class="string">'multinomial'</span>)</span><br><span class="line">clf.fit(x_train_ctv, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_ctv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.784 


/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  &quot;of iterations.&quot;, ConvergenceWarning)</code></pre><h2 id="4-WordCounts特征-朴素贝叶斯"><a href="#4-WordCounts特征-朴素贝叶斯" class="headerlink" title="4 WordCounts特征+朴素贝叶斯"></a>4 WordCounts特征+朴素贝叶斯</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br></pre></td></tr></table></figure></div>

<h3 id="4-1-导入数据"><a href="#4-1-导入数据" class="headerlink" title="4.1 导入数据"></a>4.1 导入数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())</span><br></pre></td></tr></table></figure></div>

<h3 id="4-2-标签转换为数字"><a href="#4-2-标签转换为数字" class="headerlink" title="4.2 标签转换为数字"></a>4.2 标签转换为数字</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br></pre></td></tr></table></figure></div>

<h3 id="4-3-提取的word-counts特征"><a href="#4-3-提取的word-counts特征" class="headerlink" title="4.3 提取的word counts特征"></a>4.3 提取的word counts特征</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ctv = CountVectorizer(min_df=<span class="number">3</span>,</span><br><span class="line">                      max_df=<span class="number">0.5</span>,</span><br><span class="line">                      ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Count Vectorizer来fit训练集和测试集（半监督学习）</span></span><br><span class="line">ctv.fit(text)</span><br><span class="line">text_ctv = ctv.transform(text)</span><br></pre></td></tr></table></figure></div>

<h3 id="4-4-切分数据集"><a href="#4-4-切分数据集" class="headerlink" title="4.4 切分数据集"></a>4.4 切分数据集</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_ctv, x_valid_ctv, y_train, y_valid = train_test_split(text_ctv, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="4-5-定义损失函数"><a href="#4-5-定义损失函数" class="headerlink" title="4.5 定义损失函数"></a>4.5 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="4-6-使用模型分类"><a href="#4-6-使用模型分类" class="headerlink" title="4.6 使用模型分类"></a>4.6 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用提取的word counts特征来fit一个简单的Logistic Regression </span></span><br><span class="line"></span><br><span class="line">clf = MultinomialNB()</span><br><span class="line">clf.fit(x_train_ctv, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_ctv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 3.735 </code></pre><h2 id="5-TF-IDF的SVD特征-支持向量机"><a href="#5-TF-IDF的SVD特征-支持向量机" class="headerlink" title="5 TF-IDF的SVD特征+支持向量机"></a>5 TF-IDF的SVD特征+支持向量机</h2><h3 id="5-1-数据准备"><a href="#5-1-数据准备" class="headerlink" title="5.1 数据准备"></a>5.1 数据准备</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing, decomposition</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 导入数据</span></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 标签转换为数字</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 TF-IDF提取文本特征</span></span><br><span class="line">tfv1 = TfidfVectorizer(min_df=<span class="number">4</span>,  </span><br><span class="line">                       max_df=<span class="number">0.6</span>)</span><br><span class="line">tfv1.fit(text)</span><br><span class="line">features = tfv1.transform(text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 切分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_tfv, x_valid_tfv, y_train, y_valid = train_test_split(features, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="5-2-定义损失函数"><a href="#5-2-定义损失函数" class="headerlink" title="5.2 定义损失函数"></a>5.2 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="5-3-数据标准化"><a href="#5-3-数据标准化" class="headerlink" title="5.3 数据标准化"></a>5.3 数据标准化</h3><p>由于SVM需要花费大量时间，因此在应用SVM之前，我们将使用奇异值分解（Singular Value Decomposition ）来减少TF-IDF中的特征数量。</p>
<p>同时，在使用SVM之前，我们还需要将数据标准化（Standardize Data ）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用SVD进行降维，components设为120，对于SVM来说，SVD的components的合适调整区间一般为120~200 </span></span><br><span class="line">svd = decomposition.TruncatedSVD(n_components=<span class="number">120</span>)</span><br><span class="line">svd.fit(x_train_tfv)</span><br><span class="line">xtrain_svd = svd.transform(x_train_tfv)</span><br><span class="line">xvalid_svd = svd.transform(x_valid_tfv)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对从SVD获得的数据进行缩放</span></span><br><span class="line">scl = preprocessing.StandardScaler()</span><br><span class="line">scl.fit(xtrain_svd)</span><br><span class="line">xtrain_svd_scl = scl.transform(xtrain_svd)</span><br><span class="line">xvalid_svd_scl = scl.transform(xvalid_svd)</span><br></pre></td></tr></table></figure></div>

<h3 id="5-4-使用模型分类"><a href="#5-4-使用模型分类" class="headerlink" title="5.4 使用模型分类"></a>5.4 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用下SVM模型</span></span><br><span class="line">clf = SVC(C=<span class="number">1.0</span>, probability=<span class="literal">True</span>) <span class="comment"># since we need probabilities</span></span><br><span class="line">clf.fit(xtrain_svd_scl, y_train)</span><br><span class="line">predictions = clf.predict_proba(xvalid_svd_scl)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.390 </code></pre><h2 id="6-网格搜索-朴素贝叶斯"><a href="#6-网格搜索-朴素贝叶斯" class="headerlink" title="6 网格搜索+朴素贝叶斯"></a>6 网格搜索+朴素贝叶斯</h2><p>网格搜索是一种超参数优化的技巧。 如果知道这个技巧，可以通过获取最优的参数组合来产生良好的文本分类效果。</p>
<p>下面讨论使用基于朴素贝叶斯模型的网格搜索。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span>  preprocessing,metrics, pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> TruncatedSVD</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br></pre></td></tr></table></figure></div>

<h3 id="6-1-准备数据"><a href="#6-1-准备数据" class="headerlink" title="6.1 准备数据"></a>6.1 准备数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 导入数据</span></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 标签转换为数字</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 TF-IDF提取文本特征</span></span><br><span class="line">tfv1 = TfidfVectorizer(min_df=<span class="number">4</span>,  </span><br><span class="line">                       max_df=<span class="number">0.6</span>)</span><br><span class="line">tfv1.fit(text)</span><br><span class="line">features = tfv1.transform(text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 切分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_tfv, x_valid_tfv, y_train, y_valid = train_test_split(features, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="6-2-定义损失函数"><a href="#6-2-定义损失函数" class="headerlink" title="6.2 定义损失函数"></a>6.2 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="6-3-创建评分函数"><a href="#6-3-创建评分函数" class="headerlink" title="6.3 创建评分函数"></a>6.3 创建评分函数</h3><p>在开始网格搜索之前，我们需要创建一个评分函数，这可以通过scikit-learn的make_scorer函数完成的。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=<span class="literal">False</span>, needs_proba=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="6-4-进行网格搜索"><a href="#6-4-进行网格搜索" class="headerlink" title="6.4 进行网格搜索"></a>6.4 进行网格搜索</h3><p>以朴素贝叶斯算法为例</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">nb_model = MultinomialNB()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建pipeline </span></span><br><span class="line">clf = pipeline.Pipeline([(<span class="string">'nb'</span>, nb_model)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索参数设置</span></span><br><span class="line">param_grid = &#123;<span class="string">'nb__alpha'</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网格搜索模型（Grid Search Model）初始化</span></span><br><span class="line">model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,</span><br><span class="line">                                 verbose=<span class="number">10</span>, n_jobs=<span class="number">-1</span>, iid=<span class="literal">True</span>, refit=<span class="literal">True</span>, cv=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit网格搜索模型</span></span><br><span class="line">model.fit(x_train_tfv, y_train)  <span class="comment"># 为了减少计算量，这里我们仅使用xtrain</span></span><br><span class="line">print(<span class="string">"Best score: %0.3f"</span> % model.best_score_)</span><br><span class="line">print(<span class="string">"Best parameters set:"</span>)</span><br><span class="line">best_parameters = model.best_estimator_.get_params()</span><br><span class="line"><span class="keyword">for</span> param_name <span class="keyword">in</span> sorted(param_grid.keys()):</span><br><span class="line">    print(<span class="string">"\t%s: %r"</span> % (param_name, best_parameters[param_name]))</span><br></pre></td></tr></table></figure></div>

<pre><code>Fitting 2 folds for each of 6 candidates, totalling 12 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.9s
[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    1.9s remaining:    5.7s
[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    2.0s remaining:    2.8s
[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    2.1s remaining:    1.5s


Best score: -0.560
Best parameters set:
    nb__alpha: 0.01


[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    2.2s remaining:    0.7s
[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    2.2s finished</code></pre><h3 id="6-5-用模型来分类"><a href="#6-5-用模型来分类" class="headerlink" title="6.5 用模型来分类"></a>6.5 用模型来分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用提取的TFIDF特征来fit Naive Bayes</span></span><br><span class="line">clf = MultinomialNB(alpha=<span class="number">0.01</span>)</span><br><span class="line">clf.fit(x_train_tfv, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_tfv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.520 </code></pre><h2 id="7-word2vec的词嵌入-xgboost"><a href="#7-word2vec的词嵌入-xgboost" class="headerlink" title="7 word2vec的词嵌入+xgboost"></a>7 word2vec的词嵌入+xgboost</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span>  preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure></div>

<h3 id="7-1-数据准备"><a href="#7-1-数据准备" class="headerlink" title="7.1 数据准备"></a>7.1 数据准备</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 标签转换为数字</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将每个句子切分成单个词</span></span><br><span class="line">text_s2w= [s.split() <span class="keyword">for</span> s <span class="keyword">in</span> text]</span><br></pre></td></tr></table></figure></div>

<h3 id="7-2-构建word2vec模型"><a href="#7-2-构建word2vec模型" class="headerlink" title="7.2 构建word2vec模型"></a>7.2 构建word2vec模型</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = gensim.models.Word2Vec(text_s2w,</span><br><span class="line">                               min_count=<span class="number">5</span>,</span><br><span class="line">                               workers=<span class="number">6</span>,</span><br><span class="line">                               window =<span class="number">8</span>,</span><br><span class="line">                               size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure></div>

<p>参数说明：</p>
<ul>
<li><p>min_count: 对于词频 &lt; min_count 的单词，将舍弃（其实最合适的方法是用 UNK 符号代替，即所谓的『未登录词』，这里我们简化起见，认为此类低频词不重要，直接抛弃）</p>
</li>
<li><p>workers: 可以并行执行的核心数，需要安装 Cython 才能起作用（安装 Cython 的方法很简单，直接 pip install cython）</p>
</li>
</ul>
<p>size: 词向量的维度，神经网络隐层节点数</p>
<ul>
<li>window: 目标词汇的上下文单词距目标词的最长距离，很好理解，比如 CBOW 模型是用一个词的上下文预测这个词，那这个上下文总得有个限制，如果取得太多，距离目标词太远，有些词就没啥意义了，而如果取得太少，又信息不足，所以 window 就是上下文的一个最长距离</li>
</ul>
<h3 id="7-3-word2vec模型的简单使用"><a href="#7-3-word2vec模型的简单使用" class="headerlink" title="7.3 word2vec模型的简单使用"></a>7.3 word2vec模型的简单使用</h3><h4 id="7-3-1-构建词建词嵌入字典"><a href="#7-3-1-构建词建词嵌入字典" class="headerlink" title="7.3.1 构建词建词嵌入字典"></a>7.3.1 构建词建词嵌入字典</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">embeddings_index = dict(zip(model.wv.index2word, model.wv.vectors))</span><br><span class="line">print(<span class="string">'Found %s word vectors.'</span> % len(embeddings_index))</span><br></pre></td></tr></table></figure></div>

<pre><code>Found 87117 word vectors.</code></pre><h4 id="7-3-2-获取某个词的向量"><a href="#7-3-2-获取某个词的向量" class="headerlink" title="7.3.2 获取某个词的向量"></a>7.3.2 获取某个词的向量</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model[<span class="string">'汽车'</span>]</span><br></pre></td></tr></table></figure></div>

<pre><code>/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).
  &quot;&quot;&quot;Entry point for launching an IPython kernel.

array([-2.240292  , -1.1615268 , -1.4746077 ,  2.1054246 ,  4.819405  ,
       -3.1492457 , -0.05073776, -2.1645617 , -1.2719896 ,  1.7608824 ,
       -0.2626409 , -0.64887804,  1.3482507 ,  0.34045577,  1.4765079 ,
       -3.445696  ,  1.449008  , -0.09463242,  0.6401563 , -1.6335047 ,
       -0.30473268,  2.6725786 , -0.1342183 ,  0.27526513, -2.4943345 ,
        0.27751288, -1.9030106 , -0.2115223 ,  0.48280153,  2.8040369 ,
        1.4369518 , -1.6659547 ,  0.6498365 ,  3.1322846 , -1.7274039 ,
       -0.4276681 ,  2.0273833 , -1.2563524 , -2.2891238 ,  0.80385494,
       -0.8380016 , -1.1951414 ,  0.21576834, -1.8307697 ,  1.4016038 ,
       -0.07672032,  0.97227174,  1.3520627 ,  0.568014  , -1.914469  ,
       -1.1551676 ,  0.7751831 ,  0.7154037 ,  1.2694645 ,  1.9431589 ,
       -0.06259096,  3.4280195 ,  0.6663932 , -2.665189  ,  0.6598596 ,
       -0.07868402, -0.5291124 ,  1.8237985 , -0.7853107 , -0.16555293,
       -2.074671  , -0.87207425,  0.7680195 ,  0.40575528,  0.29356548,
       -2.8064344 , -2.5557816 , -1.554487  , -2.7589092 , -0.35392886,
       -0.6011241 , -0.31734776, -1.1346784 ,  0.1052264 ,  0.57027906,
        1.1536218 ,  2.066991  , -1.1962171 ,  1.0027347 ,  0.40441233,
        2.2641828 , -2.0621223 ,  2.0815525 ,  3.5621598 , -0.4967822 ,
       -0.717848  ,  3.1545784 ,  1.1730249 ,  1.3114505 , -0.36371502,
       -0.41231316, -2.3199863 , -0.10876293, -0.44529822, -2.18213   ],
      dtype=float32)</code></pre><h4 id="7-3-3-查看某个词的与其他词的相似度"><a href="#7-3-3-查看某个词的与其他词的相似度" class="headerlink" title="7.3.3 查看某个词的与其他词的相似度"></a>7.3.3 查看某个词的与其他词的相似度</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model.most_similar(<span class="string">'人民日报'</span>)</span><br></pre></td></tr></table></figure></div>

<pre><code>/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
  &quot;&quot;&quot;Entry point for launching an IPython kernel.

[(&apos;光明日报&apos;, 0.8604782223701477),
 (&apos;海外版&apos;, 0.8062193393707275),
 (&apos;年月日&apos;, 0.7948733568191528),
 (&apos;经济日报&apos;, 0.7898619174957275),
 (&apos;文汇报&apos;, 0.7830426692962646),
 (&apos;社论&apos;, 0.7795723676681519),
 (&apos;评论员&apos;, 0.765376091003418),
 (&apos;中国作协&apos;, 0.7639801502227783),
 (&apos;讲话&apos;, 0.7555620670318604),
 (&apos;第五次&apos;, 0.7492089867591858)]</code></pre><h4 id="7-3-4-保存模型"><a href="#7-3-4-保存模型" class="headerlink" title="7.3.4 保存模型"></a>7.3.4 保存模型</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model.save(<span class="string">'/tmp/w2v_model'</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="7-3-5-加载模型"><a href="#7-3-5-加载模型" class="headerlink" title="7.3.5 加载模型"></a>7.3.5 加载模型</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model_load = gensim.models.Word2Vec.load(<span class="string">'/tmp/w2v_model'</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="7-4-训练数据处理"><a href="#7-4-训练数据处理" class="headerlink" title="7.4 训练数据处理"></a>7.4 训练数据处理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#该函数会将语句转化为一个标准化的向量（Normalized Vector）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sent2vec</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将每个句子转换会一个100的向量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words = s.split()</span><br><span class="line">    M = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#M.append(embeddings_index[w])</span></span><br><span class="line">            M.append(model[w])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    M = np.array(M)  <span class="comment"># shape=(x,100),x是句子中词的个数，100是每个词向量的维数</span></span><br><span class="line">    v = M.sum(axis=<span class="number">0</span>) <span class="comment"># 维度是100，对M中的x个数求和，得到每一维度的总和</span></span><br><span class="line">    <span class="keyword">if</span> type(v) != np.ndarray: </span><br><span class="line">        <span class="keyword">return</span> np.zeros(<span class="number">100</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v / np.sqrt((v ** <span class="number">2</span>).sum()) <span class="comment"># 正则化，最后每个句子都变为一100维的向量</span></span><br></pre></td></tr></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对训练集和验证集使用上述函数，进行文本向量化处理</span></span><br><span class="line">text_s2v = [sent2vec(s) <span class="keyword">for</span> s <span class="keyword">in</span> tqdm(text)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成numpy array数组</span></span><br><span class="line">text_s2v = np.array(text_s2v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_w2v, x_valid_w2v, y_train, y_valid = train_test_split(text_s2v, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<pre><code>  0%|          | 0/9249 [00:00&lt;?, ?it/s]/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).
  # This is added back by InteractiveShellApp.init_path()
100%|██████████| 9249/9249 [01:11&lt;00:00, 129.79it/s]</code></pre><h3 id="7-5-调用模型进行分类"><a href="#7-5-调用模型进行分类" class="headerlink" title="7.5 调用模型进行分类"></a>7.5 调用模型进行分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>


<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于word2vec特征在一个简单的Xgboost模型上进行拟合</span></span><br><span class="line">clf = xgb.XGBClassifier(max_depth=<span class="number">7</span>, n_estimators=<span class="number">200</span>, colsample_bytree=<span class="number">0.8</span>, </span><br><span class="line">                        subsample=<span class="number">0.8</span>, nthread=<span class="number">10</span>, learning_rate=<span class="number">0.1</span>, silent=<span class="literal">False</span>)</span><br><span class="line">clf.fit(x_train_w2v, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_w2v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.367 </code></pre><h2 id="8-WordCounts特征-xgboost"><a href="#8-WordCounts特征-xgboost" class="headerlink" title="8 WordCounts特征+xgboost"></a>8 WordCounts特征+xgboost</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br></pre></td></tr></table></figure></div>

<h3 id="8-1-导入数据"><a href="#8-1-导入数据" class="headerlink" title="8.1 导入数据"></a>8.1 导入数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())</span><br></pre></td></tr></table></figure></div>

<h3 id="8-2-标签转换为数字"><a href="#8-2-标签转换为数字" class="headerlink" title="8.2 标签转换为数字"></a>8.2 标签转换为数字</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br></pre></td></tr></table></figure></div>

<h3 id="8-3-提取的word-counts特征"><a href="#8-3-提取的word-counts特征" class="headerlink" title="8.3 提取的word counts特征"></a>8.3 提取的word counts特征</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ctv = CountVectorizer(min_df=<span class="number">3</span>,</span><br><span class="line">                      max_df=<span class="number">0.5</span>,</span><br><span class="line">                      ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Count Vectorizer来fit训练集和测试集（半监督学习）</span></span><br><span class="line">ctv.fit(text)</span><br><span class="line">text_ctv = ctv.transform(text)</span><br></pre></td></tr></table></figure></div>

<h3 id="8-4-切分数据集"><a href="#8-4-切分数据集" class="headerlink" title="8.4 切分数据集"></a>8.4 切分数据集</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_ctv, x_valid_ctv, y_train, y_valid = train_test_split(text_ctv, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="8-5-定义损失函数"><a href="#8-5-定义损失函数" class="headerlink" title="8.5 定义损失函数"></a>8.5 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="8-6-使用模型分类"><a href="#8-6-使用模型分类" class="headerlink" title="8.6 使用模型分类"></a>8.6 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于word counts特征，使用xgboost</span></span><br><span class="line">clf = xgb.XGBClassifier(max_depth=<span class="number">7</span>, n_estimators=<span class="number">200</span>, colsample_bytree=<span class="number">0.8</span>, </span><br><span class="line">                        subsample=<span class="number">0.8</span>, nthread=<span class="number">10</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line">clf.fit(x_train_ctv, y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_ctv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.184 </code></pre><h2 id="9-TF-IDF的SVD特征-xgboost"><a href="#9-TF-IDF的SVD特征-xgboost" class="headerlink" title="9 TF-IDF的SVD特征+xgboost"></a>9 TF-IDF的SVD特征+xgboost</h2><h3 id="9-1-数据准备"><a href="#9-1-数据准备" class="headerlink" title="9.1 数据准备"></a>9.1 数据准备</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 导入数据</span></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 标签转换为数字</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 TF-IDF提取文本特征</span></span><br><span class="line">tfv1 = TfidfVectorizer(min_df=<span class="number">4</span>,  </span><br><span class="line">                       max_df=<span class="number">0.6</span>)</span><br><span class="line">tfv1.fit(text)</span><br><span class="line">features = tfv1.transform(text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 切分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_tfv, x_valid_tfv, y_train, y_valid = train_test_split(features, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="9-2-数据标准化"><a href="#9-2-数据标准化" class="headerlink" title="9.2 数据标准化"></a>9.2 数据标准化</h3><p>由于SVM需要花费大量时间，因此在应用SVM之前，我们将使用奇异值分解（Singular Value Decomposition ）来减少TF-IDF中的特征数量。</p>
<p>同时，在使用SVM之前，我们还需要将数据标准化（Standardize Data ）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用SVD进行降维，components设为120，对于SVM来说，SVD的components的合适调整区间一般为120~200 </span></span><br><span class="line">svd = decomposition.TruncatedSVD(n_components=<span class="number">120</span>)</span><br><span class="line">svd.fit(x_train_tfv)</span><br><span class="line">xtrain_svd = svd.transform(x_train_tfv)</span><br><span class="line">xvalid_svd = svd.transform(x_valid_tfv)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对从SVD获得的数据进行缩放</span></span><br><span class="line">scl = preprocessing.StandardScaler()</span><br><span class="line">scl.fit(xtrain_svd)</span><br><span class="line">xtrain_svd_scl = scl.transform(xtrain_svd)</span><br><span class="line">xvalid_svd_scl = scl.transform(xvalid_svd)</span><br></pre></td></tr></table></figure></div>

<h3 id="9-3-定义损失函数"><a href="#9-3-定义损失函数" class="headerlink" title="9.3 定义损失函数"></a>9.3 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="9-4-使用模型分类"><a href="#9-4-使用模型分类" class="headerlink" title="9.4 使用模型分类"></a>9.4 使用模型分类</h3><h4 id="9-4-1-基于tf-idf的svd特征"><a href="#9-4-1-基于tf-idf的svd特征" class="headerlink" title="9.4.1 基于tf-idf的svd特征"></a>9.4.1 基于tf-idf的svd特征</h4><p>基于tf-idf的svd特征，使用xgboost</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf = xgb.XGBClassifier(nthread=<span class="number">10</span>)</span><br><span class="line">clf.fit(xtrain_svd, y_train)</span><br><span class="line">predictions = clf.predict_proba(xvalid_svd)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.385 </code></pre><h4 id="9-4-2-标准化的tf-idf-svd特征"><a href="#9-4-2-标准化的tf-idf-svd特征" class="headerlink" title="9.4.2 标准化的tf-idf-svd特征"></a>9.4.2 标准化的tf-idf-svd特征</h4><p>对经过数据标准化(Scaling)的tf-idf-svd特征使用xgboost</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clf = xgb.XGBClassifier(nthread=<span class="number">10</span>)</span><br><span class="line">clf.fit(xtrain_svd_scl, y_train)</span><br><span class="line">predictions = clf.predict_proba(xvalid_svd_scl)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.385 </code></pre><h2 id="10-TF-IDF-xgboost"><a href="#10-TF-IDF-xgboost" class="headerlink" title="10 TF-IDF+xgboost"></a>10 TF-IDF+xgboost</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br></pre></td></tr></table></figure></div>

<h3 id="10-1-数据准备"><a href="#10-1-数据准备" class="headerlink" title="10.1 数据准备"></a>10.1 数据准备</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 导入数据</span></span><br><span class="line">labels = []</span><br><span class="line">text = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output/data_clean_split.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    document_split = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> document_split:</span><br><span class="line">        temp = document.split(<span class="string">'\t'</span>)</span><br><span class="line">        labels.append(temp[<span class="number">0</span>])</span><br><span class="line">        text.append(temp[<span class="number">1</span>].strip())  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 标签转换为数字</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">y = label_encoder.fit_transform(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 TF-IDF提取文本特征</span></span><br><span class="line">tfv1 = TfidfVectorizer(min_df=<span class="number">4</span>,  </span><br><span class="line">                       max_df=<span class="number">0.6</span>)</span><br><span class="line">tfv1.fit(text)</span><br><span class="line">features = tfv1.transform(text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 切分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train_tfv, x_valid_tfv, y_train, y_valid = train_test_split(features, y, </span><br><span class="line">                                                  stratify=y, </span><br><span class="line">                                                  random_state=<span class="number">42</span>, </span><br><span class="line">                                                  test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="10-2-定义损失函数"><a href="#10-2-定义损失函数" class="headerlink" title="10.2 定义损失函数"></a>10.2 定义损失函数</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_logloss</span><span class="params">(actual, predicted, eps=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line">    <span class="string">"""对数损失度量（Logarithmic Loss  Metric）的多分类版本。</span></span><br><span class="line"><span class="string">    :param actual: 包含actual target classes的数组</span></span><br><span class="line"><span class="string">    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Convert 'actual' to a binary array if it's not already:</span></span><br><span class="line">    <span class="keyword">if</span> len(actual.shape) == <span class="number">1</span>:</span><br><span class="line">        actual2 = np.zeros((actual.shape[<span class="number">0</span>], predicted.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, val <span class="keyword">in</span> enumerate(actual):</span><br><span class="line">            actual2[i, val] = <span class="number">1</span></span><br><span class="line">        actual = actual2</span><br><span class="line"></span><br><span class="line">    clip = np.clip(predicted, eps, <span class="number">1</span> - eps)</span><br><span class="line">    rows = actual.shape[<span class="number">0</span>]</span><br><span class="line">    vsota = np.sum(actual * np.log(clip))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1.0</span> / rows * vsota</span><br></pre></td></tr></table></figure></div>

<h3 id="10-3-使用模型分类"><a href="#10-3-使用模型分类" class="headerlink" title="10.3 使用模型分类"></a>10.3 使用模型分类</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于tf-idf特征，使用xgboost</span></span><br><span class="line">clf = xgb.XGBClassifier(max_depth=<span class="number">7</span>, n_estimators=<span class="number">200</span>, colsample_bytree=<span class="number">0.8</span>, </span><br><span class="line">                        subsample=<span class="number">0.8</span>, nthread=<span class="number">10</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line">clf.fit(x_train_tfv.tocsc(), y_train)</span><br><span class="line">predictions = clf.predict_proba(x_valid_tfv.tocsc())</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"logloss: %0.3f "</span> % multiclass_logloss(y_valid, predictions))</span><br></pre></td></tr></table></figure></div>

<pre><code>logloss: 0.225 </code></pre></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">foochane</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://foochane.cn/article/2019091501.html">https://foochane.cn/article/2019091501.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://foochane.cn">foochane</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类    </a></div><div class="post_share"><div class="social-share" data-image data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/images/site/reward/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/images/site/reward/alipay.png" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/article/2019102201.html"><img class="prev_cover lazyload" data-src="/images/cover/7.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>pip源更换国内镜像</span></div></a></div><div class="next-post pull_right"><a href="/article/2019090301.html"><img class="next_cover lazyload" data-src="/images/cover/7.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>文本分类之CountVectorizer使用</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="https://foochane.cn/article/2019090101.html" title="文本分类之文本预处理"><img class="relatedPosts_cover lazyload"data-src="/images/cover/2.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-09-01</div><div class="relatedPosts_title">文本分类之文本预处理</div></div></a></div><div class="relatedPosts_item"><a href="https://foochane.cn/article/2019090201.html" title="文本分类之TfidfVectorizer的使用"><img class="relatedPosts_cover lazyload"data-src="/images/cover/4.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-09-02</div><div class="relatedPosts_title">文本分类之TfidfVectorizer的使用</div></div></a></div><div class="relatedPosts_item"><a href="https://foochane.cn/article/2019052202.html" title="词嵌入+神经网络进行邮件分类"><img class="relatedPosts_cover lazyload"data-src="/images/cover/2.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-05-22</div><div class="relatedPosts_title">词嵌入+神经网络进行邮件分类</div></div></a></div><div class="relatedPosts_item"><a href="https://foochane.cn/article/2019090301.html" title="文本分类之CountVectorizer使用"><img class="relatedPosts_cover lazyload"data-src="/images/cover/7.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-09-03</div><div class="relatedPosts_title">文本分类之CountVectorizer使用</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = true == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'zCeKCftRr91sIth6b8UsWKb9-gzGzoHsz',
  appKey:'SIMyV1MuzsbUpvUErd6bUlC2',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></main><footer id="footer" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2022 By foochane</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/click_heart.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>